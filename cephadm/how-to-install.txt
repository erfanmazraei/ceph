# step 1 : we have 3 nodes :
    we execute these aommands on each node :
        CEPH_RELEASE=18.2.0 # replace this with the active release
        curl --silent --remote-name --location https://download.ceph.com/rpm-${CEPH_RELEASE}/el9/noarch/cephadm
        chmod +x cephadm
        sudo ./cephadm add-repo --release reef
        apt policy cephadm
        sudo apt install cephadm

# step 2 : in node 1 :
    execute this coomand :
        cephadm bootstrap --mon-ip THIS_NODE_IP --initial-dashboard-user admin --initial-dashboard-password 123456

# step 3 : add host to cluster :
    execute this coomand :
        cat /etc/ceph/ceph.pub ( copy this to .ssh/authorized_keys other nodes )
        cephadm shell 
        ceph orch host add ceph02 188.121.112.87
        ceph orch host ls
            you see : 
                HOST                       ADDR             LABELS  STATUS  
                ceph02  188.121.112.87                   
                ceph01  188.121.113.100  _admin          
                2 hosts in cluster
        
# step 4 : add admin label :
    execute this command (in first admin node):
        cephadm shell
        ceph orch host label add ceph02 _admin
        ceph orch host ls
            you see
                HOST                       ADDR             LABELS  STATUS  
                ceph02  188.121.112.87   _admin            
                ceph01  188.121.113.100  _admin          
                2 hosts in cluster

# step 5 : see what parameter run in which node :
    execute this coomand (before this command you must execute cephadm shell):
        ceph orch ps

# step 6 : add osd :
    execute these commands :
        ceph orch apply osd --all-available-devices --dry-run (this is simulate)
        ceph orch apply osd --all-available-devices
        then new service added to cluster that names of this service is : osd.all-available-devices 
        and you can see this service by this command : 
            ceph orch ls
        and you can see added osd by execute this command : 
            ceph orch ps 
        or use this  command :
            ceph osd tree
# step 7 : see osd pool :
    execute this command:
        ceph osd pool ls 
        or 
        ceph osd pool ls detail

# step 8 : create pool with size of pg 64 :
    execute this command :
        ceph osd pool create bdpooltest 64
    see detail :
        ceph osd pool ls detail
    we must say to ceph that this pool must use as a bloack device :
        rbd pool init bdpooltest
    
# step 9 : create volume :
    execute this command :
        rbd create --size 10G --pool bdpooltest bdvolumetest

# step10 : see volumes :
    execute this command :
        rbd ls --pool bdpooltest
        or
        rbd ls --pool bdpooltest -l ( -l for see more details )

# step 11 : we must create new user and also we can see users :
    execute this command for see all users:
        ceph auth ls
    execute this command for create new user :
        ceph auth add client.test mon 'allow r' osd 'allow rwx pool=bdpooltest'
    ok see this user and permission by execute this command :
        ceph auth get client.test

# step 12 : we want use this volume in client ( new linux ) :
    execute this command :
        in this client , update your repo for install ceph-common like step 1 
        apt install ceph-common

# step 13 : copy config from server and paste to client :
    execute this command in server :
        cephadm shell
        ceph config generate-minimal-conf
        or
        cat /etc/ceph/ceph.conf
    execute this command in client :
        vim /etc/ceph/ceph.conf ( past config here )

# step 14 : add created user to client :
    execute this command :
        copy output of this command in server : 
            ceph auth get client.test 
        paste to client :
            vim /etc/ceph/client.test


# step 15 : load rbd module in client :
    execute this command :
        modprobe rbd

# step 16 : see volume in client
    execute this command :
        rbd -c /etc/ceph/ceph.conf -k /etc/ceph/client.test -n client.test ls --pool bdpooltest -l

## note : in step 14 we create this file : /etc/ceph/client.test so for list volume we must
          use -k /etc/ceph/client.test , if we rename /etc/ceph/client.test to 
          /etc/ceph/client.keyring so we use this command :
            rbd -n client.test ls --pool bdpooltest -l
            instead of step 16

# step 17 : map this volume in client :
    execute this command :
        rbd -n client.test device map --pool bdpooltest bdvolumetest
            and in output you see : /dev/rbd0
    this is not permanent , so you must do this for use this volume permanent :
        vim /etc/ceph/rbdmap
            add this line :
                bdpooltest/bdvolumetest id=test,keyring=/etc/ceph/client.keyring
                systemctl restart rbdmap.service
                ls /dev/rbd* ( you see your block device )

        
# step 18 : create file system and mount block device in client :
    execute this command :
        mkfs.ext4 /dev/rbd0
        mount /dev/rbd0 /mnt
        df -h ( you see /mnt with 10G size )

# step 19 : config fstab for permanent mount in client :
    execute this command :
        add this line to /etc/fstab :
            /dev/rbd/bdpooltest/bdvolumetest /mnt ext4 noauto 0 0
        
### note in video 13 , why we must delete image after remove files in volume ?

# step 20 : see our crush map :
    execute this command :
        ceph osd getcrushmap -o crush.e ( this is encrypted and we must decrypt this file )
        so :
            crushtool -d crush.e -o crush.d
        cat crush.d







        
