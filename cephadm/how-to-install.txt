# step 1 : we have 3 nodes :
    we execute these aommands on each node :
        CEPH_RELEASE=18.2.0 # replace this with the active release
        curl --silent --remote-name --location https://download.ceph.com/rpm-${CEPH_RELEASE}/el9/noarch/cephadm
        chmod +x cephadm
        sudo ./cephadm add-repo --release reef
        apt policy cephadm
        sudo apt install cephadm

# step 2 : in node 1 :
    execute this coomand :
        cephadm bootstrap --mon-ip THIS_NODE_IP --initial-dashboard-user admin --initial-dashboard-password 123456

# step 3 : add host to cluster :
    execute this coomand :
        cat /etc/ceph/ceph.pub ( copy this to .ssh/authorized_keys other nodes )
        cephadm shell 
        ceph orch host add ceph02 188.121.112.87
        ceph orch host ls
            you see : 
                HOST                       ADDR             LABELS  STATUS  
                ceph02  188.121.112.87                   
                ceph01  188.121.113.100  _admin          
                2 hosts in cluster
        
# step 4 : add admin label :
    execute this command (in first admin node):
        cephadm shell
        ceph orch host label add ceph02 _admin
        ceph orch host ls
            you see
                HOST                       ADDR             LABELS  STATUS  
                ceph02  188.121.112.87   _admin            
                ceph01  188.121.113.100  _admin          
                2 hosts in cluster

# step 5 : see what parameter run in which node :
    execute this coomand (before this command you must execute cephadm shell):
        ceph orch ps

# step 6 : add osd :
    execute these commands :
        ceph orch apply osd --all-available-devices --dry-run (this is simulate)
        ceph orch apply osd --all-available-devices
        then new service added to cluster that names of this service is : osd.all-available-devices 
        and you can see this service by this command : 
            ceph orch ls
        and you can see added osd by execute this command : 
            ceph orch ps 
        or use this  command :
            ceph osd tree
# step 7 : see osd pool :
    execute this command:
        ceph osd pool ls 
        or 
        ceph osd pool ls detail

# step 8 : create pool with size of pg 64 :
    execute this command :
        ceph osd pool create bdpooltest 64
    see detail :
        ceph osd pool ls detail
    we must say to ceph that this pool must use as a bloack device :
        rbd pool init bdpooltest
    
# step 9 : create volume :
    execute this command :
        rbd create --size 10G --pool bdpooltest bdvolumetest

# step10 : see volumes :
    execute this command :
        rbd ls --pool bdpooltest
        or
        rbd ls --pool bdpooltest -l ( -l for see more details )

# step 11 : we must create new user and also we can see users :
    execute this command for see all users:
        ceph auth ls
    execute this command for create new user :
        ceph auth add client.test mon 'allow r' osd 'allow rwx pool=bdpooltest'
    ok see this user and permission by execute this command :
        ceph auth get client.test

# step 12 : we want use this volume in client ( new linux ) :
    execute this command :
        in this client , update your repo for install ceph-common like step 1 
        apt install ceph-common

# step 13 : copy config from server and paste to client :
    execute this command in server :
        cephadm shell
        ceph config generate-minimal-conf
        or
        cat /etc/ceph/ceph.conf
    execute this command in client :
        vim /etc/ceph/ceph.conf ( past config here )

# step 14 : add created user to client :
    execute this command :
        copy output of this command in server : 
            ceph auth get client.test 
        paste to client :
            vim /etc/ceph/client.test


# step 15 : load rbd module in client :
    execute this command :
        modprobe rbd

# step 16 : see volume in client
    execute this command :
        rbd -c /etc/ceph/ceph.conf -k /etc/ceph/client.test -n client.test ls --pool bdpooltest -l

## note : in step 14 we create this file : /etc/ceph/client.test so for list volume we must
          use -k /etc/ceph/client.test , if we rename /etc/ceph/client.test to 
          /etc/ceph/client.keyring so we use this command :
            rbd -n client.test ls --pool bdpooltest -l
            instead of step 16

# step 17 : map this volume in client :
    execute this command :
        rbd -n client.test device map --pool bdpooltest bdvolumetest
            and in output you see : /dev/rbd0
    this is not permanent , so you must do this for use this volume permanent :
        vim /etc/ceph/rbdmap
            add this line :
                bdpooltest/bdvolumetest id=test,keyring=/etc/ceph/client.keyring
                systemctl restart rbdmap.service
                ls /dev/rbd* ( you see your block device )

        
# step 18 : create file system and mount block device in client :
    execute this command :
        mkfs.ext4 /dev/rbd0
        mount /dev/rbd0 /mnt
        df -h ( you see /mnt with 10G size )

# step 19 : config fstab for permanent mount in client :
    execute this command :
        add this line to /etc/fstab :
            /dev/rbd/bdpooltest/bdvolumetest /mnt ext4 noauto 0 0
        
### note in video 13 , why we must delete image after remove files in volume ?

# step 20 : see our crush map :
    execute this command :
        ceph osd getcrushmap -o crush.e ( this is encrypted and we must decrypt this file )
        so :
            crushtool -d crush.e -o crush.d
        cat crush.d

# step 21 : upgrade cluster
    execute this command :
        ceph orch upgrade start --ceph-version 18.2.0
        check upgrade status :
            ceph orch upgrade status
# step 22 : work with orch :
    execute this command :
        ceph orch status # see backend (cephadm) and status of cluster
        ceph orch ls # see services
        ceph orch rm grafana # remove grafana service (wee see grafana service in ceph orch ls)
        ceph orch apply grafana # create grafana again
        ceph orch apply alertmanager --placement="2 ceph01 ceph02" # we must alertmanager be in ceph01 and also ceph02
        ceph orch ps # see daemon
        ceph orch daemon start osd.8 (we see osd.8 in ceph orch ps)

# step 23 : see and change ceph config :
    execute this command :
        ceph config dump
        ceph config set mon mon_allow_pool_delete true # we must set this for delete pool

# step 24 : remove pool :
    execute this command : 
        ceph config set mon mon_allow_pool_delete true
        ceph osd pool rm POOLNAME
        ceph config set mon mon_allow_pool_delete false
# step 25 : setup rgw :
    execute this command :
        ceph orch apply rgw tgwtest
        ceph orch ls ( you see two container created for rgw.tgwtest , but in production all monoitor have a rgw container)
        ceph osd pool ls ( you can see some pool created after apply rgw like : default.rgw.log and default.rgw.control and default.rgw.meta and .rgw.root)

# step 26 : create user in server for use rgw :
    execute this command :
        radosgw-admin user create --uid=erfan --display-name="erfan mazraei" --email=erfanmazraei@gmail.com
        # in output you see some information like access key and secret key and ...

# step 27 : use storage in client :
    execute this command :
        aws configure --profile=erfan
        # and then input access key and secret key 
        # you can see information in : 
            .aws/configs
        radosgw-admin bucket list # you see nothing
        aws --profile erfan --endpoint-url http://IP_OF_HOST_THAT_RGW_CONTAINER_HERE s3 ls
        aws --profile erfan --endpoint-url http://IP_OF_HOST_THAT_RGW_CONTAINER_HERE s3 mb s3://buckettest
        radosgw-admin bucket list # you see buckettest 

# step 28 : create and delete file in bucket :
    execute this command :
        aws --profile erfan --endpoint-url http://IP_OF_HOST_THAT_RGW_CONTAINER_HERE s3api put-object --bucket buckettest --key testfile --body /etc/fstab
        aws --profile erfan --endpoint-url http://IP_OF_HOST_THAT_RGW_CONTAINER_HERE s3api delete-object --bucket buckettest --key testfile

# step 29 : see files in object :
    execute this command :
        aws --profile erfan --endpoint-url http://IP_OF_HOST_THAT_RGW_CONTAINER_HERE  s3api list-object --bucket buckettest

# step 30 : create rgw with custom pool :
    execute this command :
        firstly remove last rgw :
            ceph orch rm rgw.tgwtest
        after remove rgw , pool of this rgw not deleted and we deleted these pools manualy :
            ceph config mon  mon_allow_pool_delete true
            ceph osd pool rm .rgw.root .rgw.root(and other pool for last rgw) --yes-i-really-really-mean-it
        now apply new rgw :
            ceph orch  apply rgw test
            radosgw-admin zone get > test.json
            vim test.json
            radosgw-admin zone set < test.json
            ceph orch restart rgw.test
            # and then ceph created new pools and then you must delete older pools
# step 31 : create erasure pool for data for rgw
    execute this command :
        see all erasure profile :
            ceph osd erasure-code-profile ls # you see just default
        get config of specific erasure profile :
            ceph osd erasure-code-profile get default
        create new profile :
            ceph osd erasure-code-profile set myecp k=2 m=1 crush-failure-domain=host
        create pool with new profile :
            ceph osd pool create cluster.rgw.buckets.data erasure myecp
        see detail of pool :
            ceph osd pool ls detail
        now we must set rgw application on this pool : # application like : rbd cephfs or rgw
            ceph osd pool application enable cluster.rgw.buckets.data rgw
        see that rgw application now be set on this pool :
            ceph osd pool ls detail

# step 32 : setup cecphfs 
    # when you execute ls , your req sended to MDS and MDS interact with meta (pool) but 
      when you touch a new file , youre req directly sended to data (pool) , in chephfs we 
      have two pool : meta nad data
    execute this command :
        ceph fs volume create fstest
    see all changes in cluster :
        ceph -s  # you can see mds added in services tab and volumes added in data tab 
        ceph osd pool ls # you can see two pool added : meta and data
    create user :
        ceph fs authorize fstest client.test / rw
        ceph auth get client.test 

# step 33 : use ceph fs in client :
    execute this command :
        apt install ceph-fuse ceph-common
        vim /etc/ceph/ceph.keyring # paste output of (in ceph server : $ ceph auth get client.test)
    mount :
        ceph-fuse --id test /mnt
        cd /mnt 
        mkdir dir1 dir2 dir3

# step 34 : get more test from ceph fs in client :
    execute this command in server :
        ceph fs authorize fstest client.test1 / r /dir1 rw
        vim /etc/ceph/ceph.keyring # paste output of (in ceph server : $ ceph auth get client.test1)
        ceph-fuse --id test1 /mnt
        # you cand create directory in / but you can create in /dir1




    










        
